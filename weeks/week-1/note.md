# Week 01 — 2024-09-30 to 2024-10-06
progress till now: https://youtu.be/VGFpV3Qj4as?t=1359

## ✅ Topics Covered
- ...

## ❌ Skipped / Pending
- ...

## things to explore
- in AI world - infer, regression, supervised ML , unsupervised ML, clustering, outlier detection, 
- columns are called feature in AI world

supervised algorithm
- Regression
    - Polynomial Regression
    - Linear Regression
- Classification
    - Logistic Regression
    - Random forest
    - XG Boost
    - Decision trees
Unsupervised Algorithm
- DB Scan
- K-means
- Hierarchical clustering

###  Deep learning or Neural network
In Artifical neural network we these layers -> input layer, hidden layer 1 , 2 or n number of layers, output layer etc.

### What to use when to use :
#### Use statistical machine Learning
    -> simple features like a structured etc
    -> Smal Datasets
    -> Limited compute resources
    -> Interpretability   requirements
#### Use Deep Learning
    -> Complex features
    -> Bid datasets
    -> Images videos, Audio
    -> Enough compute resources

### Different Type of neural network Architectures
#### 1. Feed Forward neural network (FNN) -> input layer -> hidden layers -> output
#### 2. Recurrent Neural network RNN
#### 3. Transformer architecture . GPT ( Generative  Pre-trained Transformer) ex: chatGpt

### Tooling or framework for Deep Learning
1. Pytorch - meta
2. TensorFlow - Google

### Generative AI
A category of AI 




### Tools
- Python
- Pandas - numpy
- Jupyter
- dmlc XGBoost
- Scikit Learn


## 🔑 Key Insights
- ...
# How general ML Training
We give input &b output to ML Training and it will give model logic. During the Inference phase it take input and model will give output.

### AI Family Tree (0:16 – 3:01)
- AI → ML → Deep Learning → GenAI  
- Statistical AI vs neural approaches (deep learning)
- Hierarchy of concepts

Statistical AI

-sfjhkj 


### Machine Learning (3:02 – 15:54)
- ML = algorithms learn from data  
- Types:  
  - **Supervised**: labeled data (input → output)  
  - **Unsupervised**: no labels (clusters, PCA)  
- Algorithms: Linear/Logistic Regression, Decision Trees  
- Concepts: features, labels, overfitting/underfitting

### Deep Learning (15:55 – 34:17)
- Neural networks: neurons, activation functions  
- Multi-layer = "deep" networks  
- Advantages for high-dimensional data (images, text)  
- Example applications

### Generative AI (34:18 – 36:49)
- Generates new content (text, images, code)  
- Different from discriminative models  
- Use cases: Chatbots, content creation, code assistants

### Traditional AI vs GenAI (36:50 – 39:20)
- **Traditional AI**: rule-based, symbolic logic  
- **GenAI**: data-driven, large-scale models  
- Trade-offs between transparency vs flexibility

### Large Language Models (39:21 – 44:05)
- Based on transformer architectures  
- Use embeddings + attention mechanisms  
- Examples: GPT, LLaMA, Claude  
- Why scale matters

### AI Agents & Agentic AI (44:06 – 56:01)
- **Agent** = decision-making entity with a goal  
- **Agentic AI** = multiple agents working together  
- Features: planning, memory, coordination  
- Frameworks: LangGraph, CrewAI

### AI Agent vs Agentic AI vs GenAI (56:02 – end)
- **GenAI** = content generation  
- **Agent** = one intelligent unit  
- **Agentic AI** = orchestration of many agents  
- Each has unique use cases; often used together

---

## 🔑 Insights
- AI has evolved from rule-based → ML → DL → GenAI → Agentic AI.  
- Deep learning unlocked modern AI progress.  
- GenAI ≠ Agents; they complement each other.  
- LLMs are core enablers for both GenAI and Agentic AI.  

---

## 🎯 Follow-up Questions
- How do LLM embeddings differ from Word2Vec?  
- What makes an agent "intelligent" beyond just tool use?  
- Where does Agentic AI offer advantages over single-agent systems?  

---

## 🎯 Focus for Next Week
- ...

## 📂 Resources
- [Resource 1](link)
- [Resource 2](link)

## 📝 Code Snippets / Examples
```python
# Example code here
```

